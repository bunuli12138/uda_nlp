{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用隐马尔可夫模型进行英文词性标注\n",
    "\n",
    "- 步骤\n",
    "要通过该项目，你必须完成下面的第 1-3 步。第 4 步包含参考资料，帮助你进一步研究 HMM 标签器。\n",
    "\n",
    "- [第 1 步](##数据集)：查看提供的界面，以加载并访问文本语料库\n",
    "- [第 2 步](#Step-2:-Build-a-Most-Frequent-Class-tagger)： 构建最常见类别标签器并用作基准\n",
    "- [第 3 步](#Step-3:-Build-an-HMM-tagger)： 构建 HMM 词性标签器并与 MFC 基准进行比较\n",
    "- [第 4 步](#Step-4:-[Optional]-Improving-model-performance)：（可选）改进 HMM 标签器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter \"magic methods\" -- only need to be run once per kernel restart\n",
    "%load_ext autoreload\n",
    "%aimport helpers, tests\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python modules -- this cell needs to be run again if you make changes to any of the files\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from helpers import show_model, Dataset\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57340 sentences in the corpus.\n",
      "There are 45872 sentences in the training set.\n",
      "There are 11468 sentences in the testing set.\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(\"tags-universal.txt\", \"brown-universal.txt\", train_test_split=0.8)\n",
    "\n",
    "print(\"There are {} sentences in the corpus.\".format(len(data)))\n",
    "print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))\n",
    "\n",
    "assert len(data) == len(data.training_set) + len(data.testing_set), \\\n",
    "       \"The number of sentences in the training set + testing set should sum to the number of sentences in the corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "helpers.Dataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句子\n",
    "\n",
    "`Dataset.sentences` 是训练语料库中的所有句子的字典，每个句子的键都是唯一句子标识符。每个 `Sentence` 本身是一个对象，具有两个属性：一个由句子中的单词组成的元组，称为 `words`，以及一个由每个单词对应的标签组成的元组，称为 `tags`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: b100-38532\n",
      "words:\n",
      "\t('Perhaps', 'it', 'was', 'right', ';', ';')\n",
      "tags:\n",
      "\t('ADV', 'PRON', 'VERB', 'ADJ', '.', '.')\n"
     ]
    }
   ],
   "source": [
    "key = 'b100-38532'\n",
    "print(\"Sentence: {}\".format(key))\n",
    "print(\"words:\\n\\t{!s}\".format(data.sentences[key].words))\n",
    "print(\"tags:\\n\\t{!s}\".format(data.sentences[key].tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**注意：**底层可迭代序列是语料库中的**无序**句子；每次调用时，不能保证会按照相同的顺序返回句子。如果你需要按照一定的顺序访问数据，请使用 `Dataset.stream()`、`Dataset.keys`、`Dataset.X` 或 `Dataset.Y` 属性。\n",
    "</div>\n",
    "\n",
    "### 计算唯一元素的数量\n",
    "\n",
    "你可以通过 `Dataset.vocab` 访问唯一单词列表（数据集词汇表）和 `Dataset.tagset` 访问唯一标签列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 1161192 samples of 56057 unique words in the corpus.\n",
      "There are 928458 samples of 50536 unique words in the training set.\n",
      "There are 232734 samples of 25112 unique words in the testing set.\n",
      "There are 5521 words in the test set that are missing in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are a total of {} samples of {} unique words in the corpus.\"\n",
    "      .format(data.N, len(data.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the training set.\"\n",
    "      .format(data.training_set.N, len(data.training_set.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the testing set.\"\n",
    "      .format(data.testing_set.N, len(data.testing_set.vocab)))\n",
    "print(\"There are {} words in the test set that are missing in the training set.\"\n",
    "      .format(len(data.testing_set.vocab - data.training_set.vocab)))\n",
    "\n",
    "assert data.N == data.training_set.N + data.testing_set.N, \\\n",
    "       \"The number of training + test samples should sum to the total number of samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'PRT', 'NOUN', 'X', 'VERB', 'ADV', 'PRON', '.', 'CONJ', 'DET', 'NUM', 'ADP', 'ADJ'}) frozenset({'PRT', 'NOUN', 'X', 'VERB', 'ADV', 'PRON', '.', 'CONJ', 'DET', 'NUM', 'ADP', 'ADJ'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.tagset, data.training_set.tagset)\n",
    "data.tagset==data.training_set.tagset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 访问单词和标签序列\n",
    "通过 `Dataset.X` 和 `Dataset.Y` 属性可以访问数据集中每个句子的有序匹配单词和标签序列集合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ('Mr.', 'Podger', 'had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.')\n",
      "\n",
      "Labels 1: ('NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "Sentence 2: ('But', 'there', 'seemed', 'to', 'be', 'some', 'difference', 'of', 'opinion', 'as', 'to', 'how', 'far', 'the', 'board', 'should', 'go', ',', 'and', 'whose', 'advice', 'it', 'should', 'follow', '.')\n",
      "\n",
      "Labels 2: ('CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADP', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB', 'VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accessing words with Dataset.X and tags with Dataset.Y \n",
    "for i in range(2):    \n",
    "    print(\"Sentence {}:\".format(i + 1), data.X[i])\n",
    "    print()\n",
    "    print(\"Labels {}:\".format(i + 1), data.Y[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 访问（单词、标签）示例             \n",
    "`Dataset.stream()` 方法会返回一个迭代器，它会将整个语料库中的所有句子的每对单词-标签连结到一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stream (word, tag) pairs:\n",
      "\n",
      "\t ('Mr.', 'NOUN')\n",
      "\t ('Podger', 'NOUN')\n",
      "\t ('had', 'VERB')\n",
      "\t ('thanked', 'VERB')\n",
      "\t ('him', 'PRON')\n",
      "\t ('gravely', 'ADV')\n",
      "\t (',', '.')\n"
     ]
    }
   ],
   "source": [
    "# use Dataset.stream() (word, tag) samples for the entire corpus\n",
    "print(\"\\nStream (word, tag) pairs:\\n\")\n",
    "for i, pair in enumerate(data.stream()):\n",
    "    print(\"\\t\", pair)\n",
    "    if i > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于我们的基准标签器和我们将构建的 HMM 模型，我们需要根据训练语料库中的观察量频次估算标签和单词的频次。在下面的几个单元格中，你将完成计算多个计数集合的函数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 2 步：构建最常见类别标签器\n",
    "---\n",
    "\n",
    "或许最简单的标签器（以及良好的标签器性能基准）是直接选择分配给每个单词频次最高的标签。这个“最常见类别”标签器会检查在序列中观察的每个单词，并为其分配在语料库中最常分配给该单词的标签。\n",
    "\n",
    "### 实现：成对计数\n",
    "\n",
    "请完成下面的函数，该函数会计算两个输入序列的联合频次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t1\t2\t2\t3\t3\t4\t4\t5\t5\t4\t6\t4\t\n",
      "{'a': ['1', '4', '4'], 'b': ['2', '3'], 'e': ['5'], 'g': ['4']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': {'1': 1, '4': 2}, 'b': {'2': 1, '3': 1}, 'e': {'5': 1}, 'g': {'4': 1}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "l1 = ['a','b','b','a','e','a','g']\n",
    "l2 = ['1','2','3','4','5','4','4']\n",
    "\n",
    "def pair_counts(sequences_A, sequences_B):\n",
    "    # TODO: Finish this function!\n",
    "    d_total, d_lst = {}, {}\n",
    "    l = []\n",
    "    for i in range(len(sequences_A)):\n",
    "        d_lst[sequences_A[i]] = d_lst.get(sequences_A[i],[])\n",
    "        \n",
    "        d_lst[sequences_A[i]].append(sequences_B[i])\n",
    "    \n",
    "    for key,val in d_lst.items():\n",
    "        d_total[key] = dict(Counter(val))\n",
    "    return d_total\n",
    "\n",
    "# {'a':{'1':1, '4':2}, 'b':{'2':1, '3':1}, 'e':{'5':1}, 'g':{'7:1'}}\n",
    "pair_counts(l1,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Uh oh. There should be 12 tags in your dictionary.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-4d93066e6419>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memission_counts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m  \u001b[1;34m\"Uh oh. There should be 12 tags in your dictionary.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memission_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"NOUN\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0memission_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"NOUN\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m        \u001b[1;34m\"Hmmm...'time' is expected to be the most common NOUN.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Uh oh. There should be 12 tags in your dictionary."
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def pair_counts(sequences_A, sequences_B):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the first sequence list\n",
    "    that counts the number of occurrences of the corresponding value from the\n",
    "    second sequences list.\n",
    "    \n",
    "    For example, if sequences_A is tags and sequences_B is the corresponding\n",
    "    words, then if 1244 sequences contain the word \"time\" tagged as a NOUN, then\n",
    "    you should return a dictionary such that pair_counts[NOUN][time] == 1244\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function!\n",
    "    d_total, d_lst = {}, {}\n",
    "    for i in range(len(sequences_A)):\n",
    "        d_lst[sequences_A[i]] = d_lst.get(sequences_A[i],[])\n",
    "        \n",
    "        d_lst[sequences_A[i]].append(sequences_B[i])\n",
    "    \n",
    "    for key,val in d_lst.items():\n",
    "        d_total[key] = dict(Counter(val))\n",
    "    return d_total\n",
    "        \n",
    "# Calculate C(t_i, w_i)\n",
    "emission_counts = pair_counts(data.Y, data.X)\n",
    "print(emission_counts)\n",
    "\n",
    "assert len(emission_counts) == 12, \\\n",
    " \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "assert max(emission_counts[\"NOUN\"], key=emission_counts[\"NOUN\"].get) == 'time', \\\n",
    "       \"Hmmm...'time' is expected to be the most common NOUN.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现：最常见类别标签器\n",
    "\n",
    "使用 `pair_counts()` 函数和训练数据集找到训练数据中每个单词的最常见类别标签，并填充下面的 `mfc_table`。表格键应该是单词，值应该是相应的标签字符串 。\n",
    "\n",
    "`MFCTagger` 类用来模仿 Pomegranite HMM 模型的接口，使它们可以交替地使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-246aa67cd82d>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-246aa67cd82d>\"\u001b[1;36m, line \u001b[1;32m27\u001b[0m\n\u001b[1;33m    mfc_model = MFCTagger(mfc_table) # Create a Most Frequent Class tagger instance\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Create a lookup table mfc_table where mfc_table[word] contains the tag label most frequently assigned to that word\n",
    "from collections import namedtuple\n",
    "\n",
    "FakeState = namedtuple(\"FakeState\", \"name\")\n",
    "\n",
    "class MFCTagger:\n",
    "    # NOTE: You should not need to modify this class or any of its methods\n",
    "    missing = FakeState(name=\"<MISSING>\")\n",
    "    \n",
    "    def __init__(self, table):\n",
    "        self.table = defaultdict(lambda: MFCTagger.missing)\n",
    "        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})\n",
    "        \n",
    "    def viterbi(self, seq):\n",
    "        \"\"\"This method simplifies predictions by matching the Pomegranate viterbi() interface\"\"\"\n",
    "        return 0., list(enumerate([\"<start>\"] + [self.table[w] for w in seq] + [\"<end>\"]))\n",
    "\n",
    "\n",
    "# TODO: calculate the frequency of each tag being assigned to each word (hint: similar, but not\n",
    "# the same as the emission probabilities) and use it to fill the mfc_table\n",
    "\n",
    "word_counts = pair_counts(# TODO: YOUR CODE HERE)\n",
    "\n",
    "mfc_table = # TODO: YOUR CODE HERE\n",
    "\n",
    "# DO NOT MODIFY BELOW THIS LINE\n",
    "mfc_model = MFCTagger(mfc_table) # Create a Most Frequent Class tagger instance\n",
    "\n",
    "assert len(mfc_table) == len(data.training_set.vocab), \"\"\n",
    "assert all(k in data.training_set.vocab for k in mfc_table.keys()), \"\"\n",
    "assert sum(int(k not in mfc_table) for k in data.testing_set.vocab) == 5521, \"\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your MFC tagger has all the correct words!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用模型做出预测\n",
    "下面提供的辅助函数会与 Pomegranate 网络模型和模仿的 MFCTagger 交互，以通过简单的序列解码函数利用 Pomegranate 中的[缺失值](http://pomegranate.readthedocs.io/en/latest/nan.html)功能。请运行这些函数，然后运行下个单元格，看看 MFC 标签器做出的一些预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    \"\"\"Return a copy of the input sequence where each unknown word is replaced\n",
    "    by the literal string value 'nan'. Pomegranate will ignore these values\n",
    "    during computation.\n",
    "    \"\"\"\n",
    "    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    \"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 MFC 标签器进行解码的示例序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: b100-28144\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mfc_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9fdf241d31a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sentence Key: {}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predicted labels:\\n-----------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimplify_decoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmfc_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Actual labels:\\n--------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mfc_model' is not defined"
     ]
    }
   ],
   "source": [
    "for key in data.testing_set.keys[:3]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, mfc_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估模型准确率\n",
    "\n",
    "以下函数将评估 MFC 标签器在文本语料库中的所有句子集合上的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, model):\n",
    "    \"\"\"Calculate the prediction accuracy by using the model to decode each sequence\n",
    "    in the input X and comparing the prediction with the true labels in Y.\n",
    "    \n",
    "    The X should be an array whose first dimension is the number of sentences to test,\n",
    "    and each element of the array should be an iterable of the words in the sequence.\n",
    "    The arrays X and Y should have the exact same shape.\n",
    "    \n",
    "    X = [(\"See\", \"Spot\", \"run\"), (\"Run\", \"Spot\", \"run\", \"fast\"), ...]\n",
    "    Y = [(), (), ...]\n",
    "    \"\"\"\n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        \n",
    "        # The model.viterbi call in simplify_decoding will return None if the HMM\n",
    "        # raises an error (for example, if a test sentence contains a word that\n",
    "        # is out of vocabulary for the training set). Any exception counts the\n",
    "        # full sentence as an error (which makes this a conservative estimate).\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding(observations, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估 MFC 标签器的准确率\n",
    "运行下个单元格，以评估标签器在训练语料库和测试语料库上的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mfc_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-08df9625ea72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmfc_training_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmfc_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training accuracy mfc_model: {:.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmfc_training_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmfc_testing_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmfc_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"testing accuracy mfc_model: {:.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmfc_testing_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mfc_model' is not defined"
     ]
    }
   ],
   "source": [
    "mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)\n",
    "print(\"training accuracy mfc_model: {:.2f}%\".format(100 * mfc_training_acc))\n",
    "\n",
    "mfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)\n",
    "print(\"testing accuracy mfc_model: {:.2f}%\".format(100 * mfc_testing_acc))\n",
    "\n",
    "assert mfc_training_acc >= 0.955, \"Uh oh. Your MFC accuracy on the training set doesn't look right.\"\n",
    "assert mfc_testing_acc >= 0.925, \"Uh oh. Your MFC accuracy on the testing set doesn't look right.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your MFC tagger accuracy looks correct!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 3 步：构建 HMM 标签器\n",
    "---\n",
    "HMM 标签器对每个可能的标签都有一个隐藏状态，并用两个分布参数化：发射概率，表示从每个隐藏状态观察到给定**单词**的条件概率；以及转移概率， 表示在序列中的**标签**之间移动的条件概率。\n",
    "\n",
    "我们还将估计起始概率分布（每个**标签**是序列中的第一个标签的概率），以及结束概率分布（ 每个**标签**是序列中的最后一个标签的概率）。\n",
    "\n",
    "你可以根据以下部分所述的频次计算这些分布的最大似然率估计值，你将在这些部分实现计算频次的函数，并最终构建模型。HMM 模型将根据以下公式做出预测：\n",
    "\n",
    "$$t_i^n = \\underset{t_i^n}{\\mathrm{argmax}} \\prod_{i=1}^n P(w_i|t_i) P(t_i|t_{i-1})$$\n",
    "\n",
    "详情请参阅语音和语言处理[第 10 章](https://web.stanford.edu/~jurafsky/slp3/10.pdf)。\n",
    "\n",
    "### 实现：一元模型计数\n",
    "\n",
    "请完成以下函数，以估算每个符号与所有输入序列的同现频率。HMM 模型中的一元概率是根据以下公式估算的，其中 N 是输入中的总样本数。（你暂时只需计算数量。）\n",
    "\n",
    "$$P(tag_1) = \\frac{C(tag_1)}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-9757d09d24c5>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-9757d09d24c5>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    assert set(tag_unigrams.keys()) == data.training_set.tagset, \\\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def unigram_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequence list that\n",
    "    counts the number of occurrences of the value in the sequences list. The sequences\n",
    "    collection should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the tag NOUN appears 275558 times over all the input sequences,\n",
    "    then you should return a dictionary such that your_unigram_counts[NOUN] == 275558.\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function!\n",
    "    raise NotImplementedError\n",
    "\n",
    "# TODO: call unigram_counts with a list of tag sequences from the training set\n",
    "tag_unigrams = unigram_counts(# TODO: YOUR CODE HERE)\n",
    "\n",
    "assert set(tag_unigrams.keys()) == data.training_set.tagset, \\\n",
    "       \"Uh oh. It looks like your tag counts doesn't include all the tags!\"\n",
    "assert min(tag_unigrams, key=tag_unigrams.get) == 'X', \\\n",
    "       \"Hmmm...'X' is expected to be the least common class\"\n",
    "assert max(tag_unigrams, key=tag_unigrams.get) == 'NOUN', \\\n",
    "       \"Hmmm...'NOUN' is expected to be the most common class\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your tag unigrams look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现：二元计数\n",
    "\n",
    "请完成以下函数，以估算每个输入序列中每对符号的同现频率。这些计数会在 HMM 模型中用于估算频次计数中两个标签的二元概率，公式如下：$$P(tag_2|tag_1) = \\frac{C(tag_2|tag_1)}{C(tag_2)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-570fe6822b73>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-570fe6822b73>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    assert len(tag_bigrams) == 144, \\\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def bigram_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique PAIR of values in the input sequences\n",
    "    list that counts the number of occurrences of pair in the sequences list. The input\n",
    "    should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the pair of tags (NOUN, VERB) appear 61582 times, then you should\n",
    "    return a dictionary such that your_bigram_counts[(NOUN, VERB)] == 61582\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Finish this function!\n",
    "    raise NotImplementedError\n",
    "\n",
    "# TODO: call bigram_counts with a list of tag sequences from the training set\n",
    "tag_bigrams = bigram_counts(# TODO: YOUR CODE HERE)\n",
    "\n",
    "assert len(tag_bigrams) == 144, \\\n",
    "       \"Uh oh. There should be 144 pairs of bigrams (12 tags x 12 tags)\"\n",
    "assert min(tag_bigrams, key=tag_bigrams.get) in [('X', 'NUM'), ('PRON', 'X')], \\\n",
    "       \"Hmmm...The least common bigram should be one of ('X', 'NUM') or ('PRON', 'X').\"\n",
    "assert max(tag_bigrams, key=tag_bigrams.get) in [('DET', 'NOUN')], \\\n",
    "       \"Hmmm...('DET', 'NOUN') is expected to be the most common bigram.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your tag bigrams look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现：序列结束计数\n",
    "请完成以下代码，以估算以每个标签结束的序列二元概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-a415f3e3e14d>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-a415f3e3e14d>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    assert len(tag_ends) == 12, \"Uh oh. There should be 12 tags in your dictionary.\"\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def ending_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the end of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 18 sequences end with DET, then you should return a\n",
    "    dictionary such that your_starting_counts[DET] == 18\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function!\n",
    "    raise NotImplementedError\n",
    "\n",
    "# TODO: Calculate the count of each tag ending a sequence\n",
    "tag_ends = ending_counts(# TODO: YOUR CODE HERE)\n",
    "\n",
    "assert len(tag_ends) == 12, \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "assert min(tag_ends, key=tag_ends.get) in ['X', 'CONJ'], \"Hmmm...'X' or 'CONJ' should be the least common ending bigram.\"\n",
    "assert max(tag_ends, key=tag_ends.get) == '.', \"Hmmm...'.' is expected to be the most common ending bigram.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your ending tag counts look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现：基本 HMM 标签器\n",
    "使用在上面计算的一元和二元概率构建一个隐马尔可夫标签器。\n",
    "\n",
    "- 每个标签添加一个状态\n",
    "    - 应使用以下公式估算每个状态的发射概率：$P(w|t) = \\frac{C(t, w)}{C(t)}$\n",
    "- 添加一条从起始状态 `basic_model.start` 到每个标签的边\n",
    "    - 应使用以下公式估算转移概率：$P(t|start) = \\frac{C(start, t)}{C(start)}$\n",
    "- 添加一条从每个标签到结束状态 `basic_model.end` 的边\n",
    "    - 应使用以下公式估算转移概率：$P(end|t) = \\frac{C(t, end)}{C(t)}$\n",
    "- 在每对标签之间添加一条边\n",
    "    - 应使用以下公式估算转移概率：$P(t_2|t_1) = \\frac{C(t_1, t_2)}{C(t_1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "\n",
    "# TODO: create states with emission probability distributions P(word | tag) and add to the model\n",
    "# (Hint: you may need to loop & create/add new states)\n",
    "basic_model.add_states()\n",
    "\n",
    "# TODO: add edges between states for the observed transition frequencies P(tag_i | tag_i-1)\n",
    "# (Hint: you may need to loop & add transitions\n",
    "basic_model.add_transition()\n",
    "\n",
    "\n",
    "# NOTE: YOU SHOULD NOT NEED TO MODIFY ANYTHING BELOW THIS LINE\n",
    "# finalize the model\n",
    "basic_model.bake()\n",
    "\n",
    "assert all(tag in set(s.name for s in basic_model.states) for tag in data.training_set.tagset), \\\n",
    "       \"Every state in your network should use the name of the associated tag, which must be one of the training set tags.\"\n",
    "assert basic_model.edge_count() == 168, \\\n",
    "       (\"Your network should have an edge from the start node to each state, one edge between every \" +\n",
    "        \"pair of tags (states), and an edge from each state to the end node.\")\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your HMM network topology looks good!</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_training_acc = accuracy(data.training_set.X, data.training_set.Y, basic_model)\n",
    "print(\"training accuracy basic hmm model: {:.2f}%\".format(100 * hmm_training_acc))\n",
    "\n",
    "hmm_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, basic_model)\n",
    "print(\"testing accuracy basic hmm model: {:.2f}%\".format(100 * hmm_testing_acc))\n",
    "\n",
    "assert hmm_training_acc > 0.97, \"Uh oh. Your HMM accuracy on the training set doesn't look right.\"\n",
    "assert hmm_testing_acc > 0.955, \"Uh oh. Your HMM accuracy on the testing set doesn't look right.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your HMM tagger accuracy looks correct! Congratulations, you\\'ve finished the project.</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用 HMM 标签器进行解码的示例序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: b100-28144\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'basic_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d278fa641b46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sentence Key: {}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predicted labels:\\n-----------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimplify_decoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasic_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Actual labels:\\n--------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'basic_model' is not defined"
     ]
    }
   ],
   "source": [
    "for key in data.testing_set.keys[:3]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, basic_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完成项目\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "**注意：** **请保存 NOTEBOOK**，然后运行下个单元格以生成 HTML 副本。你需要压缩并提交此文件和 HTML 副本以供审阅。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!!jupyter nbconvert *.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 4 步：[可选]提高模型性能\n",
    "---\n",
    "你可以进一步增强你的标签器，使其在更大的标签集上的表现更好。更大的标签集具有更明显的数据稀疏性问题，因为相同规模的数据用更多的标签划分的话，每个标签的样本量就更少，数据中将有更多频次为零的缺失数据标签。这部分的技巧是可选内容。\n",
    "\n",
    "- [拉普拉斯平滑法](https://en.wikipedia.org/wiki/Additive_smoothing) (pseudocount)\n",
    "    拉普拉斯平滑法是指让所有观察到的计数加上一个小的非零值，以抵消未观察到的值。 \n",
    "- 回退平滑法\n",
    "    另一种平滑法技巧是在 N 元分布中为缺失值插入值。在处理数据稀疏性问题方面，此方法比拉普拉斯平滑法更高效。详情请参阅[语音和语言处理](https://web.stanford.edu/~jurafsky/slp3/)一书的第 4、9 和 10 章节。\n",
    "- 扩展为三元分布\n",
    "    HMM 标签器在此标签集上达到了 96% 以上的准确率，采用的是完整宾州树库标签集和[此](http://www.coli.uni-saarland.de/~thorsten/publications/Brants-ANLP00.pdf)论文中描述的架构。要更改你的 HMM 以达到相同的性能，则需要实现删除插值法（请参见论文），在频率表中添加三元概率，并重新实现维特比算法以考虑三种连续状态，而不是两种。\n",
    "\n",
    "### 获取具有更大标签集的 Brown 语料库\n",
    "请运行以下代码，以下载具有完整 NLTK 标签集的 brown 语料库副本。你需要在 NLTK 文档中调查可用的标签集信息，并判断提取要探索的 NLTK 标签子集的最佳方式。如果你按照在第 1 步指定的格式编写以下代码，则可以使用所有上述代码重新加载数据以进行比较。\n",
    "\n",
    "请参阅 NLTK 图书的[第 5 章](http://www.nltk.org/book/ch05.html)，详细了解可用的标签集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "training_corpus = nltk.corpus.brown\n",
    "training_corpus.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
